<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Salientia Stuff</title><link href="https://yvanhuele.github.io/" rel="alternate"></link><link href="https://yvanhuele.github.io/feeds/all.atom.xml" rel="self"></link><id>https://yvanhuele.github.io/</id><updated>2018-02-25T20:00:00-07:00</updated><entry><title>Winkelwagen</title><link href="https://yvanhuele.github.io/winkelwagen.html" rel="alternate"></link><published>2018-02-25T20:00:00-07:00</published><updated>2018-02-25T20:00:00-07:00</updated><author><name>Yannick</name></author><id>tag:yvanhuele.github.io,2018-02-25:/winkelwagen.html</id><summary type="html">&lt;h2&gt;What's Your Angle?&lt;/h2&gt;
&lt;p&gt;This week's word is the Dutch &lt;em&gt;winkelwagen&lt;/em&gt;: &lt;em&gt;winkel&lt;/em&gt; means shop or store, &lt;em&gt;wagen&lt;/em&gt; means cart or car, and together they form a shopping cart. I found the sound of the word amusing and wanted to feature it for that reason alone.&lt;/p&gt;
&lt;p&gt;I was just going to leave …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;What's Your Angle?&lt;/h2&gt;
&lt;p&gt;This week's word is the Dutch &lt;em&gt;winkelwagen&lt;/em&gt;: &lt;em&gt;winkel&lt;/em&gt; means shop or store, &lt;em&gt;wagen&lt;/em&gt; means cart or car, and together they form a shopping cart. I found the sound of the word amusing and wanted to feature it for that reason alone.&lt;/p&gt;
&lt;p&gt;I was just going to leave it at that when I began to wonder about the word &lt;em&gt;winkel&lt;/em&gt;: The Ducth word for shop didn't seem to match any word for shop that I knew in other languages. To be fair, unlike the word market (Dutch: &lt;em&gt;markt&lt;/em&gt;; German: &lt;em&gt;Markt&lt;/em&gt;; French: &lt;em&gt;marché&lt;/em&gt;; Spanish: &lt;em&gt;mercado&lt;/em&gt;), the words for shop or store seem to vary a lot by language (German: &lt;em&gt;Geschäft&lt;/em&gt;, &lt;em&gt;Laden&lt;/em&gt;; French: &lt;em&gt;magazin&lt;/em&gt;, &lt;em&gt;boutique&lt;/em&gt;; Spanish: &lt;em&gt;tienda&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Digging deeper, I learned that winkel &lt;a href="https://en.wiktionary.org/wiki/winkel#Etymology"&gt;once meant corner&lt;/a&gt;. Had I done any mathematics in German, I might have known that &lt;em&gt;Winkel&lt;/em&gt; is the German word for angle. Anyways, it seems that corner turned into corner store, then into store and lost its original meaning of corner (&lt;em&gt;hoek&lt;/em&gt; in modern Dutch).&lt;/p&gt;
&lt;h2&gt;Periwinkle Periwinkles&lt;/h2&gt;
&lt;p&gt;Winkel shares a common root with the English words &lt;em&gt;winch&lt;/em&gt; and &lt;em&gt;wink&lt;/em&gt;.  These words also share a common root with one type of &lt;em&gt;periwinkle&lt;/em&gt;, but not with its homonym. We can split periwinkles into two categories: in one category is a &lt;a href="https://en.wikipedia.org/wiki/Common_periwinkle"&gt;type of snail&lt;/a&gt; and in the other is a &lt;a href="https://en.wikipedia.org/wiki/Vinca"&gt;type of flower&lt;/a&gt; along with a &lt;a href="https://en.wikipedia.org/wiki/Periwinkle_(color)"&gt;color&lt;/a&gt; matching that of the flower. The snail is &lt;a href="https://en.wiktionary.org/wiki/periwinkle#Etymology_2"&gt;related&lt;/a&gt; to winking, but the flower &lt;a href="https://en.wiktionary.org/wiki/periwinkle#Etymology_1"&gt;is not&lt;/a&gt;.&lt;/p&gt;</content><category term="Dutch"></category><category term="etymology"></category></entry><entry><title>K-Means Clustering Part 1</title><link href="https://yvanhuele.github.io/k-means-clustering-part-1.html" rel="alternate"></link><published>2018-02-18T18:00:00-07:00</published><updated>2018-02-18T18:00:00-07:00</updated><author><name>Yannick</name></author><id>tag:yvanhuele.github.io,2018-02-18:/k-means-clustering-part-1.html</id><summary type="html">&lt;h2&gt;Just Another Review of K-Means&lt;/h2&gt;
&lt;p&gt;K-means clustering is a well-known algorithm for which there already exist many good online resources. My first introduction to K-means (as was the case with many other machine learning techniques) was from the textbook &lt;a href="http://www-bcf.usc.edu/~gareth/ISL/"&gt;&lt;em&gt;Introduction to Statistical Learning&lt;/em&gt;&lt;/a&gt;. However, a recent experience caused me to …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Just Another Review of K-Means&lt;/h2&gt;
&lt;p&gt;K-means clustering is a well-known algorithm for which there already exist many good online resources. My first introduction to K-means (as was the case with many other machine learning techniques) was from the textbook &lt;a href="http://www-bcf.usc.edu/~gareth/ISL/"&gt;&lt;em&gt;Introduction to Statistical Learning&lt;/em&gt;&lt;/a&gt;. However, a recent experience caused me to revisit this algorithm. After discussing K-means with someone who held some misconceptions about the algorithm, I decided to dig deeper into some of the details. Though I doubt those misconceptions are widely held, I wanted to address a few of them. Looking more closely at the algorithm, I learned a few new things as well -- in particular, the different initialization techniques. Instead of focusing on a few isolated aspects of K-means, it seemed more natural to wrap them together into a general discussion of the algorithm. And that's how another review of K-means clustering was born.  I wish I could say that I'm presenting K-means from an exciting new point of view that will completely redefine the way you think about the algorithm. But the truth is I mostly just had fun making some plots and wanted to share the results.&lt;/p&gt;
&lt;p&gt;I decided to split the discussion into several posts. In this first post, we'll talk about why one might use K-means clustering, discuss how to choose the number of clusters, and then give a brief overview of the algorithm. In the second part, we'll discuss different initialization techniques and the importance of running the algorithm several times with different random initializations. Then, in a third post, we'll look a little more closely at the math behind K-means and look at some data that causes trouble for K-means.&lt;/p&gt;
&lt;p&gt;The figures in this post were generated using Matplotib. All of the code can be found in &lt;a href="https://github.com/yvanhuele/yvanhuele.github.io/blob/extra-blog-materials/K-Means/K-Means%20Clustering.ipynb"&gt;this jupyter notebook&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;K-Means at a Glance&lt;/h2&gt;
&lt;p&gt;K-means clustering is an unsupervised learning algorithm which separates data into K distinct groups. As with many other unsupervised methods, the goal is to discover hidden structure within the data. In particular, K-means tries to separate data points into groups where the points in a group are closer to each other than to points in other groups.&lt;/p&gt;
&lt;p&gt;In low dimensional data (2 or 3 variables), we might be able to directly identify structure by plotting the data. For higher dimensional data, this cannot often be done nicely. Furthermore, even when we can identify structure in data by plotting it, we often want to describe the structure in a way that we can make further use of it.&lt;/p&gt;
&lt;p&gt;For example, in the plot below, we can see that the points cluster nicely into four distinct blobs, but if we want to make use of this clustering, we need a criterion for determining which points are part of each cluster.&lt;/p&gt;
&lt;p align="center"&gt;&lt;img alt="Four well-separated clusters of points" src="https://yvanhuele.github.io/images/k-means/four_blobs.png" style="max-width:100%" /&gt;&lt;/p&gt;
&lt;p&gt;We might start by noting that the points in the top-left cluster are distinguished from the rest by the property that &lt;span class="math"&gt;\(x &amp;lt; 0\)&lt;/span&gt; and &lt;span class="math"&gt;\(y &amp;gt; 6\)&lt;/span&gt;. Similar methods work for the bottom-right cluster, but the &lt;span class="math"&gt;\(x\)&lt;/span&gt;- and &lt;span class="math"&gt;\(y\)&lt;/span&gt;-coordinates of the two other clusters overlap.&lt;/p&gt;
&lt;p&gt;Thus, we turn to unsupervised learning techniques such as K-means clustering to discover structure in our data.&lt;/p&gt;
&lt;h2&gt;An Application&lt;/h2&gt;
&lt;p&gt;A common application of clustering techniques is &lt;a href="https://en.wikipedia.org/wiki/Market_segmentation"&gt;market segmentation&lt;/a&gt;. If an online shopping site has access to information about its customers such as browsing history, purchasing history, age, geographic location, it can be useful to separate its customer base into different subgroups. The purpose of clustering can simply be descriptive — to get an idea of the customer base. For example, suppose the site's customers are predominantly either college students in New England or retirees in Arizona. Looking at the average customer over the whole data set is not very helpful since, in reality, the site has few middle aged customers from Missouri. On the other hand, if the site's customers are first split into two subgroups, the average attributes of each subgroup would be more enligtening. Clustering can also be used to help in supervised learning problems. The site is unlikely to have a huge amount of purchasing data for every single one of its customers. By grouping together similar customers, the site can leverage a lot more data to determine which products a given customer is likely to buy and can target its advertisements accordingly.&lt;/p&gt;
&lt;p&gt;Some other applications of K-means are listed &lt;a href="https://en.wikipedia.org/wiki/K-means_clustering#Applications"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;An Example: Handwritten Digits&lt;/h2&gt;
&lt;p&gt;We're going to illustrate the power of K-means through a toy problem, by looking at images of handwritten digits. Specifically, we'll work with the test set of the &lt;a href="http://archive.ics.uci.edu/ml/datasets/optical+recognition+of+handwritten+digits"&gt;UCI ML Optical Recognition of Handwritten Digits Data Set&lt;/a&gt; which is included in the &lt;a href="http://scikit-learn.org/stable/datasets/index.html"&gt;scikit-learn datasets package&lt;/a&gt;. This data set consists of 1797 samples, each corresponding to the pixel intensities of an 8 by 8 pixel grayscale image of a handwritten digit. A few example images are shown below (reconstructed from 8x8 numeric arrays of pixel intensities):&lt;/p&gt;
&lt;p align="center"&gt;&lt;img alt="Examples of handwritten digits from the dataset" src="https://yvanhuele.github.io/images/k-means/example_digits.png" style="max-width:100%" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Toy Problem:&lt;/strong&gt; &lt;em&gt;Separate the digits into 3 groups of digits having similar shapes (or, more precisely, similar pixel intensity values).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We already have a good system for separating these digits into 10 classes (i.e., by digit), but it's not quite as clear how to separate them into just 3 classes. To try and solve this problem, we can apply K-means clustering with &lt;span class="math"&gt;\(K = 3\)&lt;/span&gt;, and look at some of the digits that made it into each cluster.&lt;/p&gt;
&lt;p align="center"&gt;&lt;img alt="Digits separated into three clusters" src="https://yvanhuele.github.io/images/k-means/digit_clusters.png" style="max-width:100%" /&gt;&lt;/p&gt;
&lt;p&gt;Without knowing anything about digits, the algorithm has nonetheless picked up on similarities. For example, the last cluster seems to contain most of the 0s, 4s, and 6s.&lt;/p&gt;
&lt;p&gt;For a more rigorous analysis, consider the following table which records how many instances of each digit made it into each cluster.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{array}{r|rrrrrrrrrr}
 &amp;amp; 0 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4 &amp;amp; 5 &amp;amp; 6 &amp;amp; 7 &amp;amp; 8 &amp;amp; 9 \\ \hline
1 &amp;amp; 2 &amp;amp; 38 &amp;amp; 161 &amp;amp; 167 &amp;amp; 0 &amp;amp; 72 &amp;amp; 1 &amp;amp; 0 &amp;amp; 65 &amp;amp; 146 \\
2 &amp;amp; 0 &amp;amp; 140 &amp;amp; 16 &amp;amp; 16 &amp;amp; 55 &amp;amp; 97 &amp;amp; 1 &amp;amp; 179 &amp;amp; 108 &amp;amp; 34 \\
3 &amp;amp; 176 &amp;amp; 4 &amp;amp; 0 &amp;amp; 0 &amp;amp; 126 &amp;amp; 13 &amp;amp; 179 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 \\
\end{array}
$$&lt;/div&gt;
&lt;p&gt;We see that cluster 1 contains most of the 2s, 3s, and 9s, that cluster 2 contains most of the 1s and 7s, and cluster 3 contains most of the 0s, 4s, and 6s and that the 5s and 8s are mostly split between clusters 1 and 2. Another way to see how the clusters split up the data is to use &lt;a href="https://en.wikipedia.org/wiki/Principal_component_analysis"&gt;principal component analysis&lt;/a&gt; (PCA), a method for representing high-dimensional data in 2-dimensions in such a way as to preserve as much spread as possible.&lt;/p&gt;
&lt;p align="center"&gt;&lt;img alt="Clustered digits plotted with the first 2 principal components" src="https://yvanhuele.github.io/images/k-means/digits_pca.png" style="max-width:100%" /&gt;&lt;/p&gt;
&lt;h2&gt;Restrictions&lt;/h2&gt;
&lt;p&gt;K-means clustering cannot be applied to all data sets, and even when it can be applied, it may not give great results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First, the &lt;strong&gt;data must be numerical&lt;/strong&gt;: we need to be able to compute distances between data points and to average data points. There are alternative clustering methods that can deal with categorical or mixed data types, such as K-modes or K-medoids with the &lt;a href="https://stats.stackexchange.com/a/15313"&gt;Gower metric&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The number of clusters K must be set in advance.&lt;/strong&gt; That is, K-means clustering cannot pick the number of clusters for you. We'll talk about this more in the next section.&lt;/li&gt;
&lt;li&gt;The goal of K-means is to minimize the distances between points in the same cluster. This captures a specific type of structure, but fails to capture other patterns, and &lt;strong&gt;there are numerical data sets that are not well-suited to K-means&lt;/strong&gt;. We'll discuss this in more detail in a later blog post.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;What is K?&lt;/h2&gt;
&lt;p&gt;The number of clusters, K, is a hyperparameter that must be set in advance. Often, K is chosen using domain expertise. You can also often get an idea for the number of clusters through plotting and data exploration.&lt;/p&gt;
&lt;p&gt;Let's explore this further using a real-world data set: &lt;a href="https://en.wikipedia.org/wiki/Iris_flower_data_set"&gt;Fisher's Iris dataset&lt;/a&gt;. The Iris data set one of many included in &lt;a href="http://scikit-learn.org/stable/datasets/index.html"&gt;scikit-learn's datasets package&lt;/a&gt;. The Iris data set contains 150 sets of measurements for 3 different species of Iris (50 samples from each species)&lt;/p&gt;
&lt;p&gt;One way to examine the data by plotting each variable against every other variable using a &lt;a href="https://seaborn.pydata.org/generated/seaborn.pairplot.html"&gt;pairplot&lt;/a&gt; from the seaborn library.&lt;/p&gt;
&lt;p align="center"&gt;&lt;img alt="Pair plot of Iris data without species labels" src="https://yvanhuele.github.io/images/k-means/iris_pairplot_unlabeled.png" style="max-width:100%" /&gt;&lt;/p&gt;
&lt;p&gt;Looking at these plots, we do indeed see some underlying structure. The five pair plots along with the histogram for petal length separate the data into two distinct groups. Recall though that there should be 3 different species of iris represented. What's going on?&lt;/p&gt;
&lt;p&gt;Since we have the luxury of knowing the individual labels (i.e. which iris species each point corresponds to), let's go ahead a look at these plots again after incorporating this information.&lt;/p&gt;
&lt;p align="center"&gt;&lt;img alt="Pair plot of Iris data with species labels" src="https://yvanhuele.github.io/images/k-means/iris_pairplot_labeled.png" style="max-width:100%" /&gt;&lt;/p&gt;
&lt;p&gt;We see that the data do indeed cluster into three groups, but that two of those just barely overlap.&lt;/p&gt;
&lt;p&gt;Another way of plotting high dimensional data is to use a dimensionality reduction technique such as PCA.  Below, the Iris data set is plotted using the first two principal components.&lt;/p&gt;
&lt;p align="center"&gt;&lt;img alt="Iris data plotted using 2 principal components" src="https://yvanhuele.github.io/images/k-means/iris_PCA.png" style="max-width:100%" /&gt;&lt;/p&gt;
&lt;p&gt;So, by exploring the data through visualizations and by using domain expertise, we have come up with two reasonable values of K, namely:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(K = 2\)&lt;/span&gt; seems most natural based on the various plots of the data.&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(K = 3\)&lt;/span&gt; would be an obvious choice if you knew that the data set contained specimens from three different species, but didn't have the species labels included in the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The results of performing K-means clustering with &lt;span class="math"&gt;\(K = 2\)&lt;/span&gt; and &lt;span class="math"&gt;\(K = 3\)&lt;/span&gt; are presented below.&lt;/p&gt;
&lt;p align="center"&gt;&lt;img alt="K-means clustering on Iris data with K = 2 and K = 3" src="https://yvanhuele.github.io/images/k-means/iris_clustering.png" style="max-width:100%" /&gt;&lt;/p&gt;
&lt;p&gt;Note that neither one of these solutions perfectly separates the data (either into the 3 distinct species or into setosa/non-setosa), but they come pretty close. &lt;/p&gt;
&lt;p&gt;In the absence of domain-expertise, there are a &lt;a href="https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set"&gt;variety of methods&lt;/a&gt; one can use to select the number of clusters.&lt;/p&gt;
&lt;h2&gt;A Look at the Algorithm&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;For a fine algorithm that gleans&lt;br /&gt;
Hidden groupings: two, four, maybe teens&lt;br /&gt;
Build centroid and label&lt;br /&gt;
Keep up 'til it's stable&lt;br /&gt;
That's a very rough draft of K-means&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;K-means clustering is performed by iterating two steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Use the cluster labels to recompute the &lt;a href="https://en.wikipedia.org/wiki/Centroid"&gt;cluster centroids&lt;/a&gt; (geometric centers).&lt;/li&gt;
&lt;li&gt;Use the cluster centroids to recompute the cluster labels.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each iteration of these steps reduces the within-cluster variation (i.e. readjusts the clusters so that they're more compact) and, eventually, the clusters will settle down to a stable configuration: neither the labels nor the centroids will change. A few steps of this process are illustrated in the figure below (using &lt;span class="math"&gt;\(K = 4\)&lt;/span&gt;):
Let's wrap up this section with small warning: the number of clusters is an optional parameter in the scikit-learn &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"&gt;KMeans&lt;/a&gt; function. However, this implementation is not cleverly selecting the number of clusters based on properties of the data. Instead it's just using the fixed default value of 8 clusters.&lt;/p&gt;
&lt;p align="center"&gt;&lt;img alt="a few clustering steps" src="https://yvanhuele.github.io/images/k-means/clustering_process.png" style="max-width:100%" /&gt;&lt;/p&gt;
&lt;p&gt;On the far left, we start with cluster labels (represented by the colors) which we use to compute the centroids in the second figure which are denoted by stars. This illustrates step 1. Now, notice that the labels don't seem well matched to the centroids: for example, the blue point near (0, 5) is much closer to the red centroid than the blue one.&lt;/p&gt;
&lt;p&gt;So, going from the second figure to the third, we apply step 2 and relabel each data point so that it matches the nearest centroid. Now the labels give us better looking clusters, but the old centroids are no longer centrally located. The orange star in the third figure is too far to the right; the blue star is too low. We can fix this by applying step 1 again to get the figure on the far right.&lt;/p&gt;
&lt;p&gt;In this example, further repetition of these steps won't change anything: we've achieved a stable clustering. However, in general you might need to apply steps 1 and 2 many times before getting your final clusters (and we'll see examples of this later on).&lt;/p&gt;
&lt;h2&gt;What's Next?&lt;/h2&gt;
&lt;p&gt;You may have noticed a problem with the description of K-means clustering given above: To perform step 1, we need to already have cluster labels. We can get labels from step 2, but only if we already have cluster centroids. Thus, before iterating these steps, we need to either initialize labels or initialize centroids.&lt;/p&gt;
&lt;p&gt;There are various methods for initializing the labels or the centroids. The reason you might want to consider different initialization techniques is because, even though K-means is guaranteed to converge to a stable clustering, the clustering might not be optimal. That is, different initializations can lead to different outcomes. To avoid having to restart too many times, we want initialize in a clever way. In part 2 of this series, we'll discuss several initialization methods and discuss the importance of multiple random initializations in more detail.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="clustering"></category><category term="k-means"></category></entry><entry><title>Суп</title><link href="https://yvanhuele.github.io/sup.html" rel="alternate"></link><published>2018-02-18T17:30:00-07:00</published><updated>2018-02-18T17:30:00-07:00</updated><author><name>Yannick</name></author><id>tag:yvanhuele.github.io,2018-02-18:/sup.html</id><summary type="html">&lt;h2&gt;Big Cottonwood Soup&lt;/h2&gt;
&lt;p&gt;Last week, I experienced a bit of reverse culture shock. This was all the more surprising given that I had neither left my original cultural environment nor spent any significant amount of time emerged in the culture I had seemingly adopted.&lt;/p&gt;
&lt;p&gt;I was waiting for the ski …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Big Cottonwood Soup&lt;/h2&gt;
&lt;p&gt;Last week, I experienced a bit of reverse culture shock. This was all the more surprising given that I had neither left my original cultural environment nor spent any significant amount of time emerged in the culture I had seemingly adopted.&lt;/p&gt;
&lt;p&gt;I was waiting for the ski bus and looking over the posted schedule.  After checking the return times, my eyes were drawn to the route map. Upon reading the labels &lt;em&gt;Big Cottonwood Cyn&lt;/em&gt; and &lt;em&gt;Little Cottonwood Cyn&lt;/em&gt;, I did a double-take:  It took me a full ten seconds to realize that &lt;em&gt;cyn&lt;/em&gt; is the (English) abbreviation for canyon rather than &lt;em&gt;суп&lt;/em&gt;, the Russian word for soup. With italics, they're indistinguishable, so here they are again without emphasis: cyn vs суп. Despite the small difference, having once misread the sign, I cannot unsee the soup.&lt;/p&gt;
&lt;p align="center"&gt;&lt;img alt="Cyn abbreviations in UTA ski bus map and Russian Wikipedia article on суп" src="https://yvanhuele.github.io/images/wow/cyn.png" style="max-width:90%" /&gt;
&lt;br&gt;
&lt;em style="font-size:90%"&gt;Left: &lt;a href="https://www.rideuta.com/-/media/Files/Rider-Tools/System-Maps/2017/2017_skimap_justmap.ashx?la=en"&gt;UTA ski bus map&lt;/a&gt;. Right &lt;a href="https://ru.wikipedia.org/wiki/%D0%A1%D1%83%D0%BF"&gt;Russian Wikipedia article on soup&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;The Opposite Problem&lt;/h2&gt;
&lt;p&gt;Although this is the first time I've accidentally mistaken the Latin alphabet for Cyrillic, the opposite problem — reading Cyrillic characters as Latin ones — was very common when I first started learning Russian and Ukrainian and is a problem that continues to haunt me.  Many &lt;a href="https://en.wikipedia.org/wiki/Cyrillic_alphabets#Common_letters"&gt;Cyrillic letters&lt;/a&gt; look the same as Latin letters but are pronounced differently. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;В/в sounds like V/v&lt;/li&gt;
&lt;li&gt;Н/н sounds like N/n&lt;/li&gt;
&lt;li&gt;Р/р sounds like R/r&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One of the more infamous examples is &lt;a href="https://en.wiktionary.org/wiki/pectopah"&gt;ресторан&lt;/a&gt; (restaurant, transliteration: &lt;em&gt;restoran&lt;/em&gt;), but others include север (north, transliteration: &lt;em&gt;sever&lt;/em&gt;), верх (up, transliteration: &lt;em&gt;verkh&lt;/em&gt;), and of course the СССР (USSR, which is surprisingly close to the transliteration: &lt;em&gt;SSSR&lt;/em&gt;).&lt;/p&gt;</content><category term="Russian"></category><category term="Cyrillic"></category></entry></feed>